"""
æ‰¹é‡å¤„ç†è½¨è¿¹æ•°æ® - ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¶æ„ï¼ˆå¤šè¿›ç¨‹å¹¶è¡Œï¼‰

æ¶æ„è®¾è®¡ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ä¸»è¿›ç¨‹ (Coordinator)                                        â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ æ•°æ®å‡†å¤‡è¿›ç¨‹æ±      â”‚â”€â”€â”€>â”‚ GPUè®¡ç®—è¿›ç¨‹   â”‚â”€â”€â”€>â”‚ä¿å­˜è¿›ç¨‹æ±     â”‚  â”‚
â”‚  â”‚ (nä¸ªCPU workers) â”‚    â”‚ (å•GPUæ‰¹é‡)   â”‚    â”‚(nä¸ªworkers)â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         ç”Ÿäº§è€…               æ¶ˆè´¹è€…/ç”Ÿäº§è€…         æ¶ˆè´¹è€…       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä¼˜åŠ¿ï¼š
1. å¹¶è¡Œè¯»å–CSV + æ•°æ®å‡†å¤‡ï¼ˆå……åˆ†åˆ©ç”¨å¤šæ ¸CPUï¼‰
2. GPUæ‰¹é‡è®¡ç®—ï¼ˆå……åˆ†åˆ©ç”¨48GBæ˜¾å­˜ï¼‰
3. å¹¶è¡Œä¿å­˜ç»“æœï¼ˆI/Oä¸é˜»å¡ï¼‰
4. æµæ°´çº¿å¤„ç†ï¼Œå‡å°‘ç­‰å¾…æ—¶é—´
5. âš¡ ä¼˜åŒ–ï¼šå‡†å¤‡è¿›ç¨‹ä¸åŠ è½½meshï¼ˆèŠ‚çœ80%å†…å­˜å’Œå¯åŠ¨æ—¶é—´ï¼‰
"""

import os
import sys
from pathlib import Path
import pandas as pd
import numpy as np
import trimesh
import yaml
import argparse
from datetime import datetime
import time
import gc
from multiprocessing import Process, Queue, Manager
import queue
import traceback

print("\nğŸ“¦ æ­£åœ¨å¯¼å…¥æ¨¡å—...", flush=True)

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
print("   - å¯¼å…¥ prepare_building_mesh_from_footprint...", flush=True)
from prepare_building_mesh_from_footprint import prepare_building_mesh_from_footprint

print("   - å¯¼å…¥ fetch_irradiance_data...", flush=True)
from fetch_irradiance_data import fetch_and_cache_irradiance_data, convert_to_pvlib_format

print("   - å¯¼å…¥ pv_calculator_gpu...", flush=True)
from pv_calculator_gpu import GPUAcceleratedSolarPVCalculator

print("   âœ… æ‰€æœ‰æ¨¡å—å¯¼å…¥å®Œæˆï¼\n", flush=True)


# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================
def detect_source_month(df, datetime_column='datetime'):
    """è‡ªåŠ¨æ£€æµ‹è½¨è¿¹æ•°æ®çš„æºæœˆä»½"""
    month_counts = df[datetime_column].dt.month.value_counts()
    source_month = month_counts.idxmax()
    return int(source_month)


def clone_trajectory_to_month(df, target_month, datetime_column='datetime'):
    """å°†è½¨è¿¹æ—¶é—´æˆ³å…‹éš†åˆ°ç›®æ ‡æœˆä»½"""
    original_dt = df[datetime_column]
    original_tz = original_dt.dt.tz

    try:
        new_dates = pd.to_datetime({
            'year': original_dt.dt.year,
            'month': target_month,
            'day': original_dt.dt.day,
            'hour': original_dt.dt.hour,
            'minute': original_dt.dt.minute,
            'second': original_dt.dt.second,
        }, errors='coerce')
    except Exception:
        def replace_month(dt):
            try:
                return dt.replace(month=target_month)
            except ValueError:
                return pd.NaT
        new_dates = original_dt.apply(replace_month)

    if original_tz is not None:
        new_dates = new_dates.dt.tz_localize(original_tz)

    valid_mask = new_dates.notna()
    df_cloned = df[valid_mask].copy()
    df_cloned[datetime_column] = new_dates[valid_mask]

    return df_cloned, (~valid_mask).sum()


def calculate_stats(result_df):
    """è®¡ç®—è½¨è¿¹çš„ç»Ÿè®¡ä¿¡æ¯"""
    stats = {
        'total_records': len(result_df),
        'total_energy_kwh': result_df['energy_kwh'].sum(),
        'avg_power_w': result_df['ac_power'].mean(),
        'max_power_w': result_df['ac_power'].max(),
        'shaded_ratio': result_df['is_shaded'].mean(),
        'avg_cell_temp': result_df['cell_temp'].mean(),
        'time_range': (result_df['datetime'].min(), result_df['datetime'].max()),
    }

    monthly_energy = result_df.groupby('month')['energy_kwh'].sum().to_dict()
    stats['monthly_energy_kwh'] = monthly_energy

    return stats


def parse_vehicle_range(range_value):
    """è§£æè½¦è¾†åŒºé—´é…ç½®"""
    if range_value is None:
        return None

    start, end = None, None

    if isinstance(range_value, str):
        if ':' not in range_value:
            raise ValueError("vehicle_range å­—ç¬¦ä¸²æ ¼å¼åº”ä¸º 'a:b'")
        start_str, end_str = range_value.split(':', 1)
        start = int(start_str) if start_str.strip() else None
        end = int(end_str) if end_str.strip() else None
    elif isinstance(range_value, (list, tuple)):
        if len(range_value) != 2:
            raise ValueError("vehicle_range åˆ—è¡¨/å…ƒç»„é•¿åº¦å¿…é¡»ä¸º2")
        start = int(range_value[0]) if range_value[0] is not None else None
        end = int(range_value[1]) if range_value[1] is not None else None
    else:
        raise ValueError("vehicle_range ä»…æ”¯æŒå­—ç¬¦ä¸²æˆ–åˆ—è¡¨/å…ƒç»„")

    if start is not None and start < 1:
        raise ValueError("vehicle_range èµ·å§‹ç´¢å¼•å¿…é¡»>=1")
    if end is not None and end < 1:
        raise ValueError("vehicle_range ç»“æŸç´¢å¼•å¿…é¡»>=1")
    if end is not None and start is not None and end < start:
        raise ValueError("vehicle_range ç»“æŸç´¢å¼•å¿…é¡»å¤§äºç­‰äºèµ·å§‹ç´¢å¼•")

    return start, end


def interpolate_parking_periods(df, threshold_minutes=2, resolution_minutes=1):
    """
    åœ¨GPSé—´éš”è¿‡å¤§æ—¶ï¼ˆåœè½¦æœŸé—´ï¼‰æ’å…¥æ’å€¼ç‚¹ä»¥è®¡ç®—å‘ç”µé‡

    èƒŒæ™¯ï¼š
    - è½¦è¾†åœè½¦æ—¶GPSå¯èƒ½ä¸è®°å½•æ–°ç‚¹ï¼Œä½†å¤ªé˜³èƒ½æ¿ä»åœ¨å‘ç”µ
    - å¤ªé˜³ä½ç½®éšæ—¶é—´å˜åŒ–ï¼Œé®æŒ¡æƒ…å†µä¹Ÿä¼šå˜åŒ–
    - éœ€è¦æ’å€¼ä»¥å‡†ç¡®è®¡ç®—æ¯ä¸ªæ—¶é—´æ®µçš„å‘ç”µåŠŸç‡

    å‚æ•°
    ----
    df : pd.DataFrame
        è½¨è¿¹æ•°æ®ï¼ˆå¿…é¡»å·²æŒ‰æ—¶é—´æ’åºï¼ŒåŒ…å«datetimeåˆ—ï¼‰
    threshold_minutes : float
        GPSé—´éš”è¶…è¿‡æ­¤å€¼ï¼ˆåˆ†é’Ÿï¼‰è®¤ä¸ºæ˜¯åœè½¦ï¼Œéœ€è¦æ’å€¼
    resolution_minutes : float
        æ’å€¼æ—¶é—´æ­¥é•¿ï¼ˆåˆ†é’Ÿï¼‰

    è¿”å›
    ----
    pd.DataFrame
        æ’å€¼åçš„è½¨è¿¹æ•°æ®

    ç¤ºä¾‹
    ----
    åŸå§‹æ•°æ®ï¼š10:00 â†’ 10:30ï¼ˆåœè½¦30åˆ†é’Ÿï¼‰
    æ’å€¼åï¼š10:00, 10:01, 10:02, ..., 10:29, 10:30
    """
    if len(df) < 2:
        return df

    # æ‰¾å‡ºéœ€è¦æ’å€¼çš„é—´éš”
    threshold_seconds = threshold_minutes * 60
    resolution_seconds = resolution_minutes * 60

    interpolated_rows = []

    for i in range(len(df)):
        # æ·»åŠ å½“å‰ç‚¹
        interpolated_rows.append(df.iloc[i])

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ’å€¼ï¼ˆä¸æ˜¯æœ€åä¸€ä¸ªç‚¹ï¼Œä¸”é—´éš”è¶…è¿‡é˜ˆå€¼ï¼‰
        if i < len(df) - 1:
            time_gap = (df.iloc[i + 1]['datetime'] - df.iloc[i]['datetime']).total_seconds()

            if time_gap > threshold_seconds:
                # è®¡ç®—éœ€è¦æ’å…¥çš„ç‚¹æ•°
                num_points = int(time_gap / resolution_seconds) - 1

                if num_points > 0:
                    # åˆ›å»ºæ’å€¼ç‚¹ï¼ˆä¿æŒä½ç½®ã€é€Ÿåº¦ã€è§’åº¦ä¸å˜ï¼‰
                    base_row = df.iloc[i].copy()

                    for j in range(1, num_points + 1):
                        new_row = base_row.copy()
                        # æ›´æ–°æ—¶é—´æˆ³
                        new_row['datetime'] = base_row['datetime'] + pd.Timedelta(seconds=j * resolution_seconds)
                        interpolated_rows.append(new_row)

    # åˆå¹¶æ‰€æœ‰è¡Œ
    result_df = pd.DataFrame(interpolated_rows).reset_index(drop=True)

    return result_df


# ============================================================================
# Worker Functions - ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼
# ============================================================================

def prepare_worker(task_queue, data_queue, config, mesh_path, worker_id):
    """
    æ•°æ®å‡†å¤‡Workerï¼ˆç”Ÿäº§è€…ï¼‰

    èŒè´£ï¼š
    1. ä»task_queueè·å–è½¨è¿¹æ–‡ä»¶è·¯å¾„
    2. è¯»å–CSVã€é‡é‡‡æ ·ã€å…‹éš†åˆ°12ä¸ªæœˆ
    3. å°†å‡†å¤‡å¥½çš„æ•°æ®æ”¾å…¥data_queue

    Parameters
    ----------
    task_queue : Queue
        ä»»åŠ¡é˜Ÿåˆ—ï¼ˆæ–‡ä»¶è·¯å¾„ï¼‰
    data_queue : Queue
        æ•°æ®é˜Ÿåˆ—ï¼ˆå‡†å¤‡å¥½çš„è½¨è¿¹æ•°æ®ï¼‰
    config : dict
        é…ç½®å­—å…¸
    mesh_path : str
        å»ºç­‘meshè·¯å¾„ï¼ˆæ­¤workerä¸ä½¿ç”¨ï¼Œä»…ä¿ç•™æ¥å£å…¼å®¹æ€§ï¼‰
    worker_id : int
        Workerç¼–å·ï¼ˆç”¨äºæ—¥å¿—ï¼‰
    """
    try:
        # âœ… ä¼˜åŒ–ï¼šå‡†å¤‡é˜¶æ®µä¸éœ€è¦åŠ è½½meshï¼ˆèŠ‚çœ80%å†…å­˜å’Œåˆå§‹åŒ–æ—¶é—´ï¼‰
        # âœ… ä¼˜åŒ–2ï¼šä¸é‡é‡‡æ ·ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹GPSç‚¹è®¡ç®—å®é™…æ—¶é—´é—´éš”

        while True:
            try:
                # è·å–ä»»åŠ¡ï¼ˆé˜»å¡ç­‰å¾…ï¼‰
                traj_file = task_queue.get(timeout=1)

                # æ¯’ä¸¸ä¿¡å·ï¼šç»“æŸworker
                if traj_file is None:
                    print(f"[å‡†å¤‡Worker-{worker_id}] æ”¶åˆ°ç»“æŸä¿¡å·ï¼Œé€€å‡º", flush=True)
                    break

                vehicle_id = traj_file.stem.replace('_processed', '')

                # è¯»å–è½¨è¿¹
                trajectory_df = pd.read_csv(traj_file)
                trajectory_df['datetime'] = pd.to_datetime(trajectory_df['datetime'])

                # ç¡®ä¿æ—¶åŒºç»Ÿä¸€
                if trajectory_df['datetime'].dt.tz is None:
                    trajectory_df['datetime'] = trajectory_df['datetime'].dt.tz_localize('Asia/Shanghai')
                else:
                    trajectory_df['datetime'] = trajectory_df['datetime'].dt.tz_convert('Asia/Shanghai')

                # æ£€æµ‹æºæœˆä»½å¹¶è¿‡æ»¤
                source_month = detect_source_month(trajectory_df)
                trajectory_df = trajectory_df[trajectory_df['datetime'].dt.month == source_month].copy()

                # âœ… FIX: æ¸…ç†NaNè§’åº¦å€¼ï¼ˆåœè½¦ç‚¹å¸¸ç¼ºå¤±è§’åº¦ï¼‰
                # åŸå› ï¼šåœè½¦æ—¶speed=0ï¼ŒGPSä¸è®¡ç®—heading angle
                # è§£å†³ï¼šå‰å‘å¡«å……æœ€åå·²çŸ¥æ–¹å‘ï¼Œåå‘å¡«å……èµ·å§‹ç‚¹ï¼Œæœ€åç”¨é»˜è®¤å€¼0Â°
                trajectory_df['angle'] = trajectory_df['angle'].ffill().bfill()

                # å¦‚æœæ•´åˆ—éƒ½æ˜¯NaNï¼ˆæå°‘è§ï¼‰ï¼Œä½¿ç”¨é»˜è®¤åŒ—å‘
                if trajectory_df['angle'].isna().any():
                    print(f"[å‡†å¤‡Worker-{worker_id}]    {vehicle_id}: âš ï¸  ç¼ºå°‘è§’åº¦æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å€¼0Â°", flush=True)
                    trajectory_df['angle'] = trajectory_df['angle'].fillna(0.0)

                # âœ… ä¸é‡é‡‡æ ·ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹GPSç‚¹
                # âœ… åœè½¦æœŸé—´æ’å€¼ï¼ˆè®¡ç®—åœè½¦æ—¶çš„å‘ç”µé‡ï¼‰
                # æŒ‰æ—¶é—´æ’åº
                trajectory_df = trajectory_df.sort_values('datetime').reset_index(drop=True)

                # åœè½¦æœŸé—´æ’å€¼ï¼šGPSé—´éš”>é˜ˆå€¼æ—¶ï¼Œæ’å…¥é¢å¤–ç‚¹ä»¥è®¡ç®—å‘ç”µé‡
                parking_threshold = config['computation'].get('parking_threshold_minutes', 2)
                interpolation_resolution = config['computation'].get('interpolation_resolution_minutes', 1)

                original_count = len(trajectory_df)
                trajectory_df = interpolate_parking_periods(
                    trajectory_df,
                    threshold_minutes=parking_threshold,
                    resolution_minutes=interpolation_resolution
                )
                interpolated_count = len(trajectory_df) - original_count

                if interpolated_count > 0:
                    print(f"[å‡†å¤‡Worker-{worker_id}]    {vehicle_id}: æ’å€¼ {interpolated_count} ä¸ªåœè½¦ç‚¹", flush=True)

                # âœ… éªŒè¯ï¼šæ£€æŸ¥è§’åº¦æ¸…ç†æ˜¯å¦æˆåŠŸ
                if trajectory_df['angle'].isna().any():
                    nan_count = trajectory_df['angle'].isna().sum()
                    print(f"[å‡†å¤‡Worker-{worker_id}]    {vehicle_id}: âš ï¸  ä»æœ‰{nan_count}ä¸ªNaNè§’åº¦", flush=True)

                # è®¡ç®—å®é™…æ—¶é—´é—´éš”ï¼ˆåˆ°ä¸‹ä¸€ä¸ªç‚¹çš„æ—¶é—´ï¼‰
                # âš ï¸ å…³é”®ï¼šèƒ½é‡è®¡ç®—ä¸º E = P(t) Ã— Î”tï¼Œå…¶ä¸­Î”tæ˜¯ä»tåˆ°t+1çš„æ—¶é—´
                trajectory_df['delta_t_seconds'] = (
                    trajectory_df['datetime'].shift(-1) - trajectory_df['datetime']
                ).dt.total_seconds()

                # å¤„ç†æœ€åä¸€ä¸ªç‚¹ï¼šä½¿ç”¨å‰é¢ç‚¹çš„ä¸­ä½æ•°é—´éš”ï¼ˆç¨³å¥ä¼°è®¡ï¼‰
                median_interval = trajectory_df['delta_t_seconds'].median()
                trajectory_df.loc[len(trajectory_df)-1, 'delta_t_seconds'] = median_interval

                # å…‹éš†åˆ°12ä¸ªæœˆ
                clone_to_all_months = config['computation'].get('clone_to_all_months', True)
                months_to_process = list(range(1, 13)) if clone_to_all_months else [source_month]

                all_monthly_trajs = []
                for target_month in months_to_process:
                    if target_month == source_month:
                        month_traj_df = trajectory_df.copy()
                    else:
                        month_traj_df, _ = clone_trajectory_to_month(trajectory_df, target_month)

                    if len(month_traj_df) > 0:
                        month_traj_df['month'] = target_month
                        all_monthly_trajs.append(month_traj_df)

                if not all_monthly_trajs:
                    print(f"[å‡†å¤‡Worker-{worker_id}] âš ï¸  {vehicle_id}: æ— æœ‰æ•ˆæ•°æ®", flush=True)
                    continue

                # åˆå¹¶å…¨å¹´æ•°æ®
                full_year_traj = pd.concat(all_monthly_trajs, ignore_index=True)

                # âœ… FIX: æ’åºä¿è¯æ—¶é—´å•è°ƒé€’å¢ï¼ˆpandas reindex with ffill/bfilléœ€è¦å•è°ƒç´¢å¼•ï¼‰
                full_year_traj = full_year_traj.sort_values('datetime').reset_index(drop=True)

                full_year_traj['vehicle_id'] = vehicle_id

                # è®¡ç®—æ—¥æœŸèŒƒå›´ï¼ˆç”¨äºè·å–æ°”è±¡æ•°æ®ï¼‰
                start_date = full_year_traj['datetime'].min().strftime('%Y-%m-%d')
                end_date = full_year_traj['datetime'].max().strftime('%Y-%m-%d')

                # æ”¾å…¥æ•°æ®é˜Ÿåˆ—
                data_queue.put({
                    'vehicle_id': vehicle_id,
                    'trajectory': full_year_traj,
                    'start_date': start_date,
                    'end_date': end_date,
                })

                print(f"[å‡†å¤‡Worker-{worker_id}] âœ… {vehicle_id}: {len(full_year_traj):,} records", flush=True)

                # æ¸…ç†å†…å­˜
                del trajectory_df, all_monthly_trajs, full_year_traj

            except queue.Empty:
                continue
            except Exception as e:
                print(f"[å‡†å¤‡Worker-{worker_id}] âŒ å¤„ç†å¤±è´¥: {e}", flush=True)
                traceback.print_exc()
                continue

    except Exception as e:
        print(f"[å‡†å¤‡Worker-{worker_id}] âŒ Workeråˆå§‹åŒ–å¤±è´¥: {e}", flush=True)
        traceback.print_exc()


def gpu_worker(data_queue, result_queue, config, mesh_path):
    """
    GPUè®¡ç®—Workerï¼ˆæ¶ˆè´¹è€…/ç”Ÿäº§è€…ï¼‰

    èŒè´£ï¼š
    1. ä»data_queueæ¶ˆè´¹å‡†å¤‡å¥½çš„è½¨è¿¹æ•°æ®
    2. ç´¯ç§¯åˆ°æŒ‡å®šæ‰¹æ¬¡å¤§å°åï¼Œæ‰¹é‡GPUè®¡ç®—
    3. å°†è®¡ç®—ç»“æœæ”¾å…¥result_queue

    Parameters
    ----------
    data_queue : Queue
        æ•°æ®é˜Ÿåˆ—ï¼ˆå‡†å¤‡å¥½çš„è½¨è¿¹ï¼‰
    result_queue : Queue
        ç»“æœé˜Ÿåˆ—ï¼ˆè®¡ç®—å®Œæˆçš„ç»“æœï¼‰
    config : dict
        é…ç½®å­—å…¸
    mesh_path : str
        å»ºç­‘meshè·¯å¾„
    """
    try:
        # åœ¨å­è¿›ç¨‹ä¸­åˆå§‹åŒ–GPUï¼ˆé¿å…forké—®é¢˜ï¼‰
        print(f"[GPU Worker] æ­£åœ¨åˆå§‹åŒ–...", flush=True)

        building_mesh = trimesh.load(mesh_path)

        calculator = GPUAcceleratedSolarPVCalculator(
            lon_center=config['location']['lon'],
            lat_center=config['location']['lat'],
            building_mesh=building_mesh,
            panel_area=config['pv_system']['panel_area'],
            panel_efficiency=config['pv_system']['panel_efficiency'],
            time_resolution_minutes=config['computation']['time_resolution_minutes'],
            use_gpu=config['computation']['use_gpu']
        )

        print(f"[GPU Worker] âœ… åˆå§‹åŒ–å®Œæˆ", flush=True)

        # æ‰¹æ¬¡ç´¯ç§¯
        batch_data = []
        gpu_batch_size = config['computation'].get('gpu_batch_size', 50)
        batch_timeout = config['computation'].get('batch_timeout', 10)

        # æ°”è±¡æ•°æ®ç¼“å­˜ï¼ˆå…¨å¹´æ•°æ®å¯å¤ç”¨ï¼‰
        weather_cache = {}

        while True:
            try:
                # å°è¯•è·å–æ•°æ®ï¼ˆå¸¦è¶…æ—¶ï¼‰
                data = data_queue.get(timeout=batch_timeout)

                # æ¯’ä¸¸ä¿¡å·ï¼šå¤„ç†å‰©ä½™æ‰¹æ¬¡åé€€å‡º
                if data is None:
                    print(f"[GPU Worker] æ”¶åˆ°ç»“æŸä¿¡å·ï¼Œå¤„ç†å‰©ä½™æ‰¹æ¬¡...", flush=True)
                    if batch_data:
                        process_batch(batch_data, calculator, weather_cache, result_queue, config)
                    break

                batch_data.append(data)

                # è¾¾åˆ°æ‰¹æ¬¡å¤§å°ï¼Œç«‹å³å¤„ç†
                if len(batch_data) >= gpu_batch_size:
                    print(f"[GPU Worker] æ‰¹æ¬¡å·²æ»¡({len(batch_data)}è¾†)ï¼Œå¼€å§‹GPUè®¡ç®—...", flush=True)
                    process_batch(batch_data, calculator, weather_cache, result_queue, config)
                    batch_data = []

            except queue.Empty:
                # è¶…æ—¶ï¼šå¦‚æœæœ‰ç§¯ç´¯çš„æ•°æ®ï¼Œä¹Ÿè¿›è¡Œå¤„ç†
                if batch_data:
                    print(f"[GPU Worker] è¶…æ—¶è§¦å‘ï¼Œå¤„ç†å½“å‰æ‰¹æ¬¡({len(batch_data)}è¾†)...", flush=True)
                    process_batch(batch_data, calculator, weather_cache, result_queue, config)
                    batch_data = []
                continue

    except Exception as e:
        print(f"[GPU Worker] âŒ Workerå¤±è´¥: {e}", flush=True)
        traceback.print_exc()


def process_batch(batch_data, calculator, weather_cache, result_queue, config):
    """
    å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡çš„è½¦è¾†æ•°æ®

    Parameters
    ----------
    batch_data : list
        æ‰¹æ¬¡æ•°æ®åˆ—è¡¨
    calculator : GPUAcceleratedSolarPVCalculator
        GPUè®¡ç®—å™¨
    weather_cache : dict
        æ°”è±¡æ•°æ®ç¼“å­˜
    result_queue : Queue
        ç»“æœé˜Ÿåˆ—
    config : dict
        é…ç½®å­—å…¸
    """
    try:
        calc_start = time.time()

        # åˆå¹¶æ‰¹æ¬¡ä¸­æ‰€æœ‰è½¦è¾†çš„è½¨è¿¹
        merged_traj = pd.concat([d['trajectory'] for d in batch_data], ignore_index=True)

        # âœ… FIX: æ’åºä¿è¯datetimeå•è°ƒé€’å¢ï¼ˆreindexè¦æ±‚ï¼‰
        # è¯´æ˜ï¼šåˆå¹¶å¤šè½¦åæ—¶é—´ä¼šå€’é€€ï¼ˆè½¦Açš„12æœˆ â†’ è½¦Bçš„1æœˆï¼‰
        # æ’åºåï¼šè½¦è¾†æ•°æ®ä¼šäº¤é”™ä½†å„è‡ªæ—¶é—´é¡ºåºä¸å˜ï¼Œæœ€åç”¨vehicle_idåˆ†ç»„è¿˜åŸ
        merged_traj = merged_traj.sort_values('datetime').reset_index(drop=True)

        # æ¨æ–­æ—¥æœŸèŒƒå›´ï¼ˆå–æ‰€æœ‰è½¦è¾†çš„æœ€å°/æœ€å¤§ï¼‰
        all_start_dates = [pd.to_datetime(d['start_date']) for d in batch_data]
        all_end_dates = [pd.to_datetime(d['end_date']) for d in batch_data]
        start_date = min(all_start_dates).strftime('%Y-%m-%d')
        end_date = max(all_end_dates).strftime('%Y-%m-%d')

        # è·å–æ°”è±¡æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰
        cache_key = f"{start_date}_{end_date}"
        if cache_key not in weather_cache:
            irradiance_data = fetch_and_cache_irradiance_data(
                lat=config['location']['lat'],
                lon=config['location']['lon'],
                start_date=start_date,
                end_date=end_date,
                granularity='1min' if config['computation']['time_resolution_minutes'] == 1 else '1hour',
                save_csv=False,
                output_dir='irradiance_data'
            )
            weather_cache[cache_key] = convert_to_pvlib_format(irradiance_data)

        weather_data = weather_cache[cache_key]

        # æ‰¹é‡GPUè®¡ç®—
        batch_result = calculator.process_trajectory(
            merged_traj,
            weather_data=weather_data,
            skip_resample=True,  # å·²åœ¨å‡†å¤‡é˜¶æ®µé‡é‡‡æ ·
            vehicle_height=config['pv_system']['vehicle_height']  # âœ… ä¼ å…¥è½¦è¾†é«˜åº¦
        )

        calc_time = time.time() - calc_start

        # æ‹†åˆ†ç»“æœå¹¶æ”¾å…¥ç»“æœé˜Ÿåˆ—
        for vehicle_id, vehicle_result in batch_result.groupby('vehicle_id'):
            vehicle_result = vehicle_result.drop(columns=['vehicle_id'])

            result_queue.put({
                'vehicle_id': vehicle_id,
                'result': vehicle_result,
                'calc_time': calc_time / len(batch_data)  # å‡æ‘Šæ—¶é—´
            })

        print(f"[GPU Worker] âœ… æ‰¹æ¬¡å®Œæˆ: {len(batch_data)}è¾†, {calc_time:.1f}s "
              f"(avg {calc_time/len(batch_data):.1f}s/vehicle)", flush=True)

        # æ¸…ç†
        del merged_traj, batch_result
        gc.collect()

    except Exception as e:
        print(f"[GPU Worker] âŒ æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}", flush=True)
        traceback.print_exc()


def save_worker(result_queue, config, stats_dict, worker_id):
    """
    ç»“æœä¿å­˜Workerï¼ˆæ¶ˆè´¹è€…ï¼‰

    èŒè´£ï¼š
    1. ä»result_queueè·å–è®¡ç®—ç»“æœ
    2. ä¿å­˜ä¸ºCSVæ–‡ä»¶
    3. æ”¶é›†ç»Ÿè®¡ä¿¡æ¯

    Parameters
    ----------
    result_queue : Queue
        ç»“æœé˜Ÿåˆ—
    config : dict
        é…ç½®å­—å…¸
    stats_dict : Manager.dict
        å…±äº«ç»Ÿè®¡å­—å…¸
    worker_id : int
        Workerç¼–å·
    """
    try:
        output_dir = Path(config['output']['output_dir'])

        while True:
            try:
                # è·å–ç»“æœï¼ˆé˜»å¡ç­‰å¾…ï¼‰
                result_data = result_queue.get(timeout=1)

                # æ¯’ä¸¸ä¿¡å·ï¼šç»“æŸworker
                if result_data is None:
                    print(f"[ä¿å­˜Worker-{worker_id}] æ”¶åˆ°ç»“æŸä¿¡å·ï¼Œé€€å‡º", flush=True)
                    break

                vehicle_id = result_data['vehicle_id']
                vehicle_result = result_data['result']
                calc_time = result_data['calc_time']

                # ä¿å­˜CSV
                result_csv = output_dir / f"{vehicle_id}_pv_generation.csv"
                vehicle_result.to_csv(result_csv, index=False)

                # è®¡ç®—ç»Ÿè®¡
                stats = calculate_stats(vehicle_result)

                # å­˜å…¥å…±äº«å­—å…¸
                stats_dict[vehicle_id] = {
                    'stats': stats,
                    'elapsed_time': calc_time
                }

                # æ˜¾ç¤ºæœˆåº¦ç»Ÿè®¡
                month_stats = []
                for month in sorted(stats['monthly_energy_kwh'].keys()):
                    month_energy = stats['monthly_energy_kwh'][month]
                    month_stats.append(f"{month:02d}:{month_energy:.1f}kWh")

                print(f"[ä¿å­˜Worker-{worker_id}] âœ… {vehicle_id}: "
                      f"{stats['total_energy_kwh']:.1f}kWh ({', '.join(month_stats[:3])}...)",
                      flush=True)

                # æ¸…ç†
                del vehicle_result

            except queue.Empty:
                continue
            except Exception as e:
                print(f"[ä¿å­˜Worker-{worker_id}] âŒ ä¿å­˜å¤±è´¥: {e}", flush=True)
                traceback.print_exc()
                continue

    except Exception as e:
        print(f"[ä¿å­˜Worker-{worker_id}] âŒ Workerå¤±è´¥: {e}", flush=True)
        traceback.print_exc()


# ============================================================================
# é…ç½®å‚æ•°
# ============================================================================
CONFIG = {
    'location': {
        'name': 'æ·±åœ³å¸‚',
        'lat': 22.543099,
        'lon': 114.057868,
    },
    'data_sources': {
        'footprint_path': 'data/shenzhen_buildings.geojson',
        'trajectory_dir': '../../../../data2/hcr/evipv/shenzhendata/taxi/taxi/processed',
    },
    'pv_system': {
        'panel_area': 2.2,
        'panel_efficiency': 0.20,
        'tilt': 0,
        'vehicle_height': 1.5,
    },
    'computation': {
        'time_resolution_minutes': 1,
        'use_gpu': True,
        'gpu_id': 1,
        'mesh_grid_size': None,
        'clone_to_all_months': True,  # âœ… å…‹éš†åˆ°12ä¸ªæœˆä»¥è®¡ç®—å…¨å¹´å‘ç”µé‡
        'max_vehicles': 1050,
        'vehicle_range': None,

        # åœè½¦æœŸé—´æ’å€¼å‚æ•°
        'parking_threshold_minutes': 2,        # GPSé—´éš”è¶…è¿‡2åˆ†é’Ÿè®¤ä¸ºåœè½¦ï¼Œéœ€æ’å€¼
        'interpolation_resolution_minutes': 1, # åœè½¦æœŸé—´æ¯1åˆ†é’Ÿæ’å€¼ä¸€ä¸ªç‚¹

        # ç”Ÿäº§è€…-æ¶ˆè´¹è€…å‚æ•°
        'num_prepare_workers': 5,    # æ•°æ®å‡†å¤‡å¹¶è¡Œè¿›ç¨‹æ•°
        'num_save_workers': 5,       # ç»“æœä¿å­˜å¹¶è¡Œè¿›ç¨‹æ•°
        'gpu_batch_size': 100,       # GPUæ‰¹é‡å¤„ç†è½¦è¾†æ•°
        'queue_maxsize': 100,        # é˜Ÿåˆ—æœ€å¤§é•¿åº¦
        'batch_timeout': 10,         # æ‰¹æ¬¡è¶…æ—¶(ç§’)
    },
    'output': {
        'mesh_path': 'data/shenzhen_building_mesh.ply',
        'output_dir': 'output',
    },
}

# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================
def main():
    parser = argparse.ArgumentParser(
        description='ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¶æ„æ‰¹é‡å¤„ç†è½¨è¿¹æ•°æ®ï¼ˆå¹¶è¡ŒI/O + CPU + æ‰¹é‡GPUï¼‰'
    )
    parser.add_argument('--config', '-c', default=None)
    parser.add_argument('--gpu', '-g', type=int, default=None)
    parser.add_argument('--vehicle-range', '-r', type=str, default=None)
    parser.add_argument('--prepare-workers', '-p', type=int, default=None,
                        help='æ•°æ®å‡†å¤‡å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤4ï¼‰')
    parser.add_argument('--save-workers', '-s', type=int, default=None,
                        help='ç»“æœä¿å­˜å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤2ï¼‰')
    parser.add_argument('--gpu-batch-size', '-b', type=int, default=None,
                        help='GPUæ‰¹é‡å¤„ç†è½¦è¾†æ•°ï¼ˆé»˜è®¤50ï¼‰')

    args = parser.parse_args()

    print("\n" + "="*80)
    print(" "*8 + "ğŸš€ Producer-Consumer Architecture PV Calculation")
    print(" "*15 + "Parallel I/O + CPU + Batch GPU")
    print("="*80)
    print(f"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    # åŠ è½½é…ç½®
    if args.config:
        if Path(args.config).exists():
            with open(args.config, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
        else:
            print(f"âš ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨å†…éƒ¨CONFIG")
            config = CONFIG
    else:
        config = CONFIG

    # GPUè®¾ç½®
    gpu_id = args.gpu if args.gpu is not None else config['computation'].get('gpu_id', 0)
    if config['computation']['use_gpu'] and gpu_id is not None:
        os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)
        print(f"ğŸ® GPU: {gpu_id}")

    # è¿›ç¨‹æ•°è®¾ç½®
    num_prepare = args.prepare_workers or config['computation'].get('num_prepare_workers', 4)
    num_save = args.save_workers or config['computation'].get('num_save_workers', 2)
    gpu_batch = args.gpu_batch_size or config['computation'].get('gpu_batch_size', 50)

    print(f"âš™ï¸  Architecture Configuration:")
    print(f"   Prepare Workers: {num_prepare} (parallel CPU)")
    print(f"   GPU Batch Size: {gpu_batch} vehicles")
    print(f"   Save Workers: {num_save} (parallel I/O)")

    # å‡†å¤‡è¾“å‡ºç›®å½•
    output_dir = Path(config['output']['output_dir'])
    output_dir.mkdir(parents=True, exist_ok=True)

    # å‡†å¤‡å»ºç­‘mesh
    print("\n" + "="*80)
    print("Preparing Building Mesh")
    print("="*80)

    mesh_path = Path(config['output']['mesh_path'])
    if mesh_path.exists():
        print(f"â±ï¸  Loading mesh: {mesh_path}")
        building_mesh = trimesh.load(mesh_path)
        print(f"   âœ… Mesh loaded")
    else:
        print(f"â±ï¸  Generating mesh...")
        building_mesh = prepare_building_mesh_from_footprint(
            footprint_path=config['data_sources']['footprint_path'],
            output_mesh_path=str(mesh_path),
            grid_size=config['computation']['mesh_grid_size']
        )
        print(f"   âœ… Mesh generated")

    # æŸ¥æ‰¾è½¨è¿¹æ–‡ä»¶
    traj_dir = Path(config['data_sources']['trajectory_dir'])
    traj_files = sorted(traj_dir.glob('*_processed.csv'))

    # è½¦è¾†ç­›é€‰
    if args.vehicle_range:
        vehicle_range = parse_vehicle_range(args.vehicle_range)
        if vehicle_range:
            start_idx, end_idx = vehicle_range
            start_0based = (start_idx - 1) if start_idx else 0
            end_0based = end_idx if end_idx else len(traj_files)
            traj_files = traj_files[start_0based:end_0based]
            print(f"\nğŸ“Œ Vehicle Range: [{start_idx if start_idx else 1}:{end_idx if end_idx else len(traj_files)}]")
    elif config['computation'].get('max_vehicles'):
        max_v = config['computation']['max_vehicles']
        traj_files = traj_files[:max_v]

    print(f"âœ… Found {len(traj_files)} vehicles to process")

    # ========== å¯åŠ¨ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¶æ„ ==========
    print("\n" + "="*80)
    print(f"Starting Producer-Consumer Pipeline")
    print("="*80 + "\n")

    start_time = time.time()

    # åˆ›å»ºé˜Ÿåˆ—
    queue_maxsize = config['computation'].get('queue_maxsize', 100)
    task_queue = Queue()  # ä»»åŠ¡é˜Ÿåˆ—ï¼ˆæ–‡ä»¶è·¯å¾„ï¼‰
    data_queue = Queue(maxsize=queue_maxsize)  # æ•°æ®é˜Ÿåˆ—ï¼ˆå‡†å¤‡å¥½çš„è½¨è¿¹ï¼‰
    result_queue = Queue(maxsize=queue_maxsize)  # ç»“æœé˜Ÿåˆ—ï¼ˆè®¡ç®—ç»“æœï¼‰

    # åˆ›å»ºå…±äº«ç»Ÿè®¡å­—å…¸
    manager = Manager()
    stats_dict = manager.dict()

    # å¡«å……ä»»åŠ¡é˜Ÿåˆ—
    for traj_file in traj_files:
        task_queue.put(traj_file)

    # æ·»åŠ æ¯’ä¸¸ä¿¡å·ï¼ˆè®©prepare workersçŸ¥é“ä½•æ—¶åœæ­¢ï¼‰
    for _ in range(num_prepare):
        task_queue.put(None)

    print(f"ğŸ“‹ Task queue filled: {len(traj_files)} vehicles")

    # å¯åŠ¨æ•°æ®å‡†å¤‡è¿›ç¨‹æ± ï¼ˆç”Ÿäº§è€…ï¼‰
    prepare_processes = []
    for i in range(num_prepare):
        p = Process(
            target=prepare_worker,
            args=(task_queue, data_queue, config, str(mesh_path), i)
        )
        p.start()
        prepare_processes.append(p)
    print(f"ğŸ­ Started {num_prepare} prepare workers")

    # å¯åŠ¨GPUè®¡ç®—è¿›ç¨‹ï¼ˆæ¶ˆè´¹è€…/ç”Ÿäº§è€…ï¼‰
    gpu_process = Process(
        target=gpu_worker,
        args=(data_queue, result_queue, config, str(mesh_path))
    )
    gpu_process.start()
    print(f"âš¡ Started GPU worker")

    # å¯åŠ¨ç»“æœä¿å­˜è¿›ç¨‹æ± ï¼ˆæ¶ˆè´¹è€…ï¼‰
    save_processes = []
    for i in range(num_save):
        p = Process(
            target=save_worker,
            args=(result_queue, config, stats_dict, i)
        )
        p.start()
        save_processes.append(p)
    print(f"ğŸ’¾ Started {num_save} save workers\n")

    print("="*80)
    print("Pipeline Running... (Ctrl+C to stop)")
    print("="*80 + "\n")

    # ç­‰å¾…æ‰€æœ‰å‡†å¤‡è¿›ç¨‹å®Œæˆ
    for p in prepare_processes:
        p.join()
    print(f"\nâœ… All prepare workers finished")

    # å‘é€æ¯’ä¸¸ç»™GPUè¿›ç¨‹
    data_queue.put(None)
    gpu_process.join()
    print(f"âœ… GPU worker finished")

    # å‘é€æ¯’ä¸¸ç»™ä¿å­˜è¿›ç¨‹
    for _ in range(num_save):
        result_queue.put(None)

    for p in save_processes:
        p.join()
    print(f"âœ… All save workers finished")

    total_time = time.time() - start_time

    # ========== è¾“å‡ºæ±‡æ€» ==========
    print("\n" + "="*80)
    print("Processing Summary")
    print("="*80)

    if stats_dict:
        all_stats = dict(stats_dict)
        total_energy = sum(s['stats']['total_energy_kwh'] for s in all_stats.values())

        print(f"\nâœ… Successfully Processed: {len(all_stats)} vehicles")
        print(f"   Total Energy (Full Year): {total_energy:.2f} kWh")
        print(f"   Wall Time: {total_time:.1f}s ({total_time/60:.1f} min)")
        print(f"   Avg Time/Vehicle: {total_time/len(all_stats):.2f}s")

        # æŒ‰æœˆæ±‡æ€»
        monthly_totals = {}
        for vehicle_id, data in all_stats.items():
            for month, energy in data['stats']['monthly_energy_kwh'].items():
                monthly_totals[month] = monthly_totals.get(month, 0) + energy

        print(f"\nğŸ“Š Monthly Energy Summary (All Vehicles):")
        for month in sorted(monthly_totals.keys()):
            print(f"   Month {month:02d}: {monthly_totals[month]:.2f} kWh")
    else:
        print("âŒ No vehicles processed successfully")

    print("\n" + "="*80)
    print("âœ… All Processing Complete!")
    print(f"End Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80 + "\n")

    return 0


if __name__ == "__main__":
    exit(main())
